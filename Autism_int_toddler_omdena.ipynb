{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67078e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\arjun\\\\omdena\\\\Autism_prediction_omdena'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, os, sklearn, plotly.express as px\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a56069d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git', '.ipynb_checkpoints', 'README.md', 'Untitled.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64bf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57786291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('...\\\\AUTISM_SCREENING_FOR_TODDLERS\\\\archive')\n",
    "df = pd.read_csv('Toddler Autism dataset July 2018.csv')\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "#############################    EXPLORATORY DATA ANALYSIS    #####################################\n",
    "#############################    EXPLORATORY DATA ANALYSIS    #####################################\n",
    "#############################    EXPLORATORY DATA ANALYSIS    #####################################\n",
    "###################################################################################################\n",
    "\n",
    "df.columns\n",
    "df.head()\n",
    "df.rename(columns ={\"Class/ASD Traits \":\"target\"}, inplace=True)\n",
    "df.head()\n",
    "df.target = np.where(df.target =='Yes',1,0)\n",
    "df.head()\n",
    "yes = df[df['target']==1]\n",
    "yes.describe()\n",
    "no = df[df['target']==0]\n",
    "no.describe()\n",
    "\n",
    "\"\"\"\n",
    "The minimum value of Qchat-10-Score variable for yes(1) (autistic) is 4.\n",
    "The maximum value of Qchat-10-Score variable for no(0) (non-autistic) is 3.\n",
    "\"\"\"\n",
    "df1 = df[['target','Qchat-10-Score']]\n",
    "df1.describe()\n",
    "\n",
    "df1.corr()# 81 % correlation\n",
    "\n",
    "df1_yes = df1[df1['Qchat-10-Score'] >3]\n",
    "df1_no = df1[df1['Qchat-10-Score'] <=3]\n",
    "\n",
    "df1_yes.describe()\n",
    "df1_no.describe()\n",
    "\n",
    "############################\n",
    "df1_yes.target.unique()\n",
    "df1_no.target.unique()\n",
    "\"\"\"\n",
    "The result shows that for:\n",
    "    Qchat-10-Score >3 target variable is 1 i.e. patient has autism.\n",
    "    Qchat-10-Score <=3 target variable is 0 i.e. patient has autism.\n",
    "    \n",
    "So by just using this information i.e. with just the Qchat-10-Score variable \n",
    "and the cut of value of 3, we can correctly predict whether a toddler has autism \n",
    "or not. This is a classical case of data leakage that is the independent \n",
    "variable contains information about the target variable hence when ever we train\n",
    "the model if the train test split is done in such a way that the model recognizes\n",
    "this information then we will almost everytime get a very high score. We may be\n",
    "happy that our model is doing so well. In real lif when new dataset comes it may\n",
    "not contain this information in the Qchat-10-Score variable and hence our model\n",
    "may not perform well. \n",
    "To check this let us :\n",
    "    1.first train the model with the Qchat-10-Score variable on the toddler \n",
    "    dataset and validate our model on the git hub data set. \n",
    "    2. secondly train the model without the Qchat-10-Score variable on the\n",
    "    toddler dataset and validate our model on the git hub data set.\n",
    "    3. thirdly train the model with the Qchat-10-Score variable on the combined \n",
    "    dataset and validate our model on the combined data set. \n",
    "    $. fourthly train the model without the Qchat-10-Score variable on the combined \n",
    "    dataset and validate our model on the combined data set. \n",
    "\"\"\"\n",
    "\n",
    "df1_yes.target.value_counts()\n",
    "df1_no.target.value_counts()\n",
    "\n",
    "def data_preprocess(df):\n",
    "    #Get the new dataset from github\n",
    "    os.chdir(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\archive\\github_data\\Data-Analytics-model-on-Behavioural-Challenges-of-ASD-kids')\n",
    "    os.listdir()\n",
    "    dff = pd.read_csv('data_csv.csv')\n",
    "    dff['Sex'] = np.where(dff['Sex']=='F','f','m')\n",
    "    dff1 = dff[[ 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8','A9', 'A10_Autism_Spectrum_Quotient','Age_Years', 'Qchat_10_Score',  'Sex', 'Ethnicity', 'Jaundice','Family_mem_with_ASD', 'Who_completed_the_test', 'ASD_traits']]\n",
    "    #max age in toddlers dataset is 36 months i.e. 3 years\n",
    "    df.Age_Mons.max()\n",
    "    #filter from the new data all records with 'Age_Years' <= 3\n",
    "    dff2 = dff1[dff1['Age_Years']<=3]\n",
    "    yes = dff2[dff2['ASD_traits']=='Yes']\n",
    "    yes.describe()\n",
    "    no = dff2[dff2['ASD_traits']=='No']\n",
    "    a = df[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10','Age_Mons', 'Qchat-10-Score', 'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD', 'Who completed the test', 'target']]\n",
    "    b = dff2\n",
    "    b.columns\n",
    "    b.rename(columns={'A10_Autism_Spectrum_Quotient':'A10'},inplace = True)\n",
    "    b.rename(columns = {'ASD_traits':'target'},inplace = True)\n",
    "    b['Age_Mons']=b.Age_Years*12\n",
    "    #b.drop(['Qchat_10_Score','Age_Years'],axis=1,inplace = True)\n",
    "    b.drop(['Age_Years'],axis=1,inplace = True)\n",
    "    b.target = np.where(b['target'] == 'Yes',1,0)\n",
    "    a.rename(columns = {'Who completed the test':'Who_completed_the_test'},inplace = True)\n",
    "    a.rename(columns = {'Qchat-10-Score':'Qchat_10_Score'},inplace = True)\n",
    "    b = b[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons','Qchat_10_Score','Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD','Who_completed_the_test', 'target']]\n",
    "    b.columns\n",
    "    a.columns\n",
    "    b['Qchat_10_Score'] = b['Qchat_10_Score'].fillna(0).astype(np.int64)\n",
    "    a['dataset']='toddler'\n",
    "    b['dataset']='github'\n",
    "    c=a.append(b)\n",
    "    ### save the preprocessed files\n",
    "    a.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\original_data.csv',index = False)\n",
    "    b.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\github_data.csv',index = False)\n",
    "    c.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data.csv',index = False)\n",
    "\n",
    "#data_preprocess(df)\n",
    "\n",
    "################################################ Read the files saved from before ###########################\n",
    "aa = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\original_data.csv')\n",
    "bb = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\github_data.csv')\n",
    "cc = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data.csv')\n",
    "\n",
    "\n",
    "aa.columns\n",
    "bb.columns\n",
    "cc.columns\n",
    "\n",
    "## Create dummy variable for categorical variables\n",
    "\n",
    "def get_dummy(df):\n",
    "    #Introducing dummy variables for all categorical variables by dropping the first dummy variable\n",
    "    Sex = pd.get_dummies(df.Sex, prefix='Sex', drop_first=True)\n",
    "    Ethnicity = pd.get_dummies(df.Ethnicity, prefix='Ethnicity', drop_first=True)\n",
    "    Jaundice = pd.get_dummies(df.Jaundice, prefix='Jaundice', drop_first=True)\n",
    "    Family_mem_with_ASD = pd.get_dummies(df.Family_mem_with_ASD, prefix='Family_mem_with_ASD', drop_first=True)\n",
    "    Who_completed_the_test = pd.get_dummies(df[\"Who_completed_the_test\"], prefix='Who_completed_the_test', drop_first=True)\n",
    "    #Introducing dummy variables for all categorical variables by dropping the first dummy variable\n",
    "    df.drop([\"Sex\",\"Ethnicity\",\"Jaundice\",\"Family_mem_with_ASD\",\"Who_completed_the_test\"], axis = 1,inplace=True)\n",
    "    df =  pd.concat([df, Sex,Ethnicity,Jaundice,Family_mem_with_ASD,Who_completed_the_test ], axis=1)\n",
    "    return(df)\n",
    "\n",
    "def save_files():\n",
    "    aaa = get_dummy(aa)\n",
    "    bbb = get_dummy(bb)\n",
    "    ccc = get_dummy(cc)\n",
    "    aaa.columns\n",
    "    bbb.columns\n",
    "    ccc.columns\n",
    "\n",
    "    aaa.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\original_data_one_hot_encoded.csv',index = False)\n",
    "    bbb.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\github_data_one_hot_encoded.csv',index = False)\n",
    "    ccc.to_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv',index = False)\n",
    "\n",
    "#save_files()\n",
    "\n",
    "###########################################################################################################################\n",
    "################################################        END OF EXPLORATORY DATA ANALYSIS       ###########################\n",
    "################################################        END OF EXPLORATORY DATA ANALYSIS       ###########################\n",
    "################################################        END OF EXPLORATORY DATA ANALYSIS       ###########################\n",
    "###########################################################################################################################\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "################################################ Read the files saved one hot encoded files  ###########################\n",
    "aa = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\original_data_one_hot_encoded.csv')\n",
    "bb = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\github_data_one_hot_encoded.csv')\n",
    "cc = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "\n",
    "\n",
    "aa.columns\n",
    "bb.columns\n",
    "cc.columns\n",
    "\n",
    "aa.shape # (1054, 30)\n",
    "bb.shape # (147, 32)\n",
    "cc.shape # (147, 32)\n",
    "\n",
    "## define functions to train the models\n",
    "def train_model1(df1,test_size):\n",
    "    df1.drop('dataset',axis = 1, inplace=True)\n",
    "    # Putting feature variable to X\n",
    "    X = df1.drop(['target'], axis=1)\n",
    "    # Puttting response variable to y\n",
    "    y = df1.loc[:,['target']]\n",
    "\n",
    "    # Splitting the data into train and test with test size as 30% and random state as 101\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= test_size)\n",
    "    # Pipeline Estimator \n",
    "    standardscaler =StandardScaler()\n",
    "    radomforestclassifier = RandomForestClassifier(n_jobs = -1,verbose = 0)\n",
    "    pipeline = make_pipeline(standardscaler,radomforestclassifier)\n",
    "    # fit model on training data\n",
    "    pipeline.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    # Predict the sales of the test data\n",
    "    y_test['pred'] = pipeline.predict(X_test)\n",
    "    from sklearn import metrics\n",
    "    # testing score\n",
    "    score = metrics.f1_score(y_test['target'], y_test['pred'],labels=None, pos_label=1)\n",
    "\n",
    "    print(\"F1 score for test data is : \",score)\n",
    "    print(\"Accuracy score for test data is : \",metrics.accuracy_score(y_test['target'], y_test['pred']))\n",
    "    print('train_test_split_ratio is : ', test_size)\n",
    "\n",
    "\n",
    "def train_model2(df1,df2):\n",
    "    # Putting feature variable to X\n",
    "    X_train = df1.drop(['target'], axis=1)\n",
    "    # Puttting response variable to y\n",
    "    y_train = df1.loc[:,['target']]\n",
    "\n",
    "\n",
    "    # Putting feature variable to X\n",
    "    X_test = df2.drop(['target'], axis=1)\n",
    "    # Puttting response variable to y\n",
    "    y_test = df2.loc[:,['target']]\n",
    "\n",
    "\n",
    "    standardscaler =StandardScaler()\n",
    "    radomforestclassifier = RandomForestClassifier(n_jobs = -1,verbose = 0)\n",
    "    pipeline = make_pipeline(standardscaler,radomforestclassifier)\n",
    "    # fit model on training data\n",
    "    pipeline.fit(X_train,y_train)\n",
    "\n",
    "    #Test data is the github dataset\n",
    "    X_test.columns\n",
    "\n",
    "\n",
    "    # Predict the sales of the test data\n",
    "    y_test['pred'] = pipeline.predict(X_test)\n",
    "    from sklearn import metrics\n",
    "    # testing score\n",
    "    score = metrics.f1_score(y_test['target'], y_test['pred'],labels=None, pos_label=1)\n",
    "\n",
    "    print(\"F1 score for test data is : \",score)\n",
    "    print(\"Accuracy score for test data is : \",metrics.accuracy_score(y_test['target'], y_test['pred']))\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "# 1. First train the model with the Qchat-10-Score variable on the toddler \n",
    "#    dataset and validate our model on the test data of the toddler data set. \n",
    "###########################################################################################################################\n",
    "df1 = pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\original_data_one_hot_encoded.csv')\n",
    "train_model1(df1, test_size=0.25)\n",
    "\n",
    "#Result\n",
    "\"\"\"\n",
    "F1 score for test data is :  1.0\n",
    "Accuracy score for test data is :  1.0\n",
    "\"\"\"\n",
    "\n",
    "###########################################################################################################################\n",
    "# 2. Secondly train the model with the Qchat-10-Score variable on the toddler \n",
    "#    dataset and validate our model on the git hub data set. \n",
    "###########################################################################################################################\n",
    "\n",
    "df =  pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "df1 = df[df['dataset']=='toddler']\n",
    "df2 = df[df['dataset']=='github']\n",
    "df1.drop('dataset',axis = 1, inplace=True)\n",
    "df2.drop('dataset',axis = 1, inplace=True)\n",
    "\n",
    "train_model2(df1, df2)\n",
    "#Result\n",
    "\"\"\"\n",
    "F1 score for test data is :  0.8488372093023255\n",
    "Accuracy score for test data is :  0.8231292517006803\n",
    "\"\"\"\n",
    "###########################################################################################################################\n",
    "# 3. Thirdly train the model without the Qchat-10-Score variable on the toddler \n",
    "#    dataset and validate our model on the git hub data set. \n",
    "###########################################################################################################################\n",
    "\n",
    "df =  pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "df.drop('Qchat_10_Score',axis = 1, inplace=True)\n",
    "df1 = df[df['dataset']=='toddler']\n",
    "df2 = df[df['dataset']=='github']\n",
    "df1.drop('dataset',axis = 1, inplace=True)\n",
    "df2.drop('dataset',axis = 1, inplace=True)\n",
    "\n",
    "train_model2(df1, df2)\n",
    "# Result\n",
    "\"\"\"\n",
    "F1 score for test data is :  0.9466666666666668\n",
    "Accuracy score for test data is :  0.9455782312925171\n",
    "\"\"\"\n",
    "###########################################################################################################################\n",
    "# 4. Fourthly train the model with the Qchat-10-Score variable on the combined \n",
    "#    dataset and validate our model on the combined data set. \n",
    "###########################################################################################################################\n",
    "\n",
    "for i in range(10,50):\n",
    "    df1 =  pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "    train_model1(df1, test_size=i*0.01)\n",
    "    print(\" Value of i is :\", i)\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "# 5. Fifth train the model without the Qchat-10-Score variable on the combined \n",
    "#    dataset and validate our model on the combined data set. \n",
    "###########################################################################################################################\n",
    "\n",
    "for i in range(10,50):\n",
    "    df1 =  pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "    df1.drop('Qchat_10_Score',axis = 1, inplace=True)\n",
    "    train_model1(df1, test_size=i*0.01)\n",
    "    print(\" Value of i is :\", i)\n",
    "    \n",
    "###########################################################################################################33\n",
    "\n",
    "\n",
    "cc =  pd.read_csv(r'...\\AUTISM_SCREENING_FOR_TODDLERS\\data\\combined_data_one_hot_encoded.csv')\n",
    "\n",
    "\n",
    "\n",
    "cc.columns\n",
    "\n",
    "df = cc[['Qchat_10_Score','target']]\n",
    "df_1 = df[df['target']==1]\n",
    "df_0 = df[df['target']==0]\n",
    "\n",
    "df_1.describe()\n",
    "df_0.describe()\n",
    "cc.target.value_counts()\n",
    "\n",
    "df_1['Qchat_10_Score'].value_counts()\n",
    "df_0['Qchat_10_Score'].value_counts()\n",
    "\n",
    "\n",
    "#################################################################\n",
    "\n",
    "### Further actions to investigate.\n",
    "# Remove outliers from 'Qchat_10_Score' variable for both yes(1) and no (0).\n",
    "# Read the criteria for the scoring 'Qchat_10_Score' variable in both the datasets and see if we can do any further data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadb095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb677d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f49011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee30bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
